{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem solve here is to classify grayscale images of hand written digits (28 × 28 pixels) into their 10 categories\n",
    "(0 through 9) using the MNIST dataset. It’s a set of 60,000 training images, plus 10,000 test images, assembled by the \n",
    "National Institute of Standards and Technology\n",
    "\n",
    "\n",
    "2.2. The network architecture \n",
    "The core building block of neural networks is the layer,  a data-processing module that Some data goes in, and it comes out in a more useful form. layers extract representations out of the data fed into them. representations that are more meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers that will implement a form of progressive data distillation. A deep-learning model is like a sieve for data processing,\n",
    "made of a succession of increasingly refined data filtersthe layers.\n",
    "\n",
    "To make the network ready for training, we need to pick three more things, as part of the compilation step:\n",
    "1} A loss function — How the network will be able to measure its performance on the training data, and thus how it will  be able to steer itself in the right direction.\n",
    "2} An optimizer — The mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "3} Metrics to monitor during training and testing — Here, we’ll only care about accuracy (the fraction of the images that were correctly classified)\n",
    "\n",
    "2.3. The compilation step\n",
    "\n",
    "2.4. Preparing the image data\n",
    "preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. our training images stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1.\n",
    "\n",
    "2.5. Preparing the labels\n",
    "categorically encode the labels\n",
    "\n",
    "2.6. Fit the model \n",
    "Two quantities are displayed during training: the loss of the network over the training data, and the accuracy of the network over the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satish\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "################ 1. Loading the MNIST dataset in Keras #####################\n",
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# train_images.shape ### dimension of tesndor along with each axes in this case (60000, 28, 28)\n",
    "# len(train_labels)\n",
    "# train_labels\n",
    "# test_labels\n",
    "print(train_images.ndim) ### Display number of axes, in this case 3\n",
    "# print(train_images.dtype) ### type of data conatain in tensor, in this case uint8\n",
    "\n",
    "################ 2. The network architecture #####################\n",
    "# network = models.Sequential()\n",
    "# network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,))) \n",
    "# ### Dense layers are fully connected neural layers.\n",
    "# network.add(layers.Dense(10, activation='softmax')) \n",
    "# ### 10-way softmax layer, i.e. it will return an array of 10 probability scores (summing to 1).\n",
    "    \n",
    "################ 3. The compilation step #########################\n",
    "\n",
    "### To make the network ready for training, we need to pick three things, as part of the\n",
    "### compilation step:\n",
    "### 1) optimizer - mechanism through which the network will update itself based on the data it\n",
    "###                sees and its loss function.\n",
    "### 2) Loss function - the network will be able to measure its performance on the training data and \n",
    "###                   steer itself in the right direction.\n",
    "### 3) Metrics ( to monitor during training and testing) - in this case only care about accuracy\n",
    "###             i.e. the fraction of the images that were correctly classified\n",
    "\n",
    "# network.compile(optimizer='rmsprop', \n",
    "#                   loss='categorical_crossentropy', \n",
    "#                   metrics=['accuracy']) \n",
    "\n",
    "################ 4. Preparing the image data #########################\n",
    "\n",
    "\n",
    "### Transform training array into float32 array of shape (60000,28 *28) with values betn 0 and 1.\n",
    "# train_images = train_images.reshape((60000, 28 * 28))\n",
    "# train_images = train_images.astype('float32')/255\n",
    "\n",
    "# test_images = test_images.reshape((10000, 28 * 28))\n",
    "# test_images = test_images.astype('float32')/255\n",
    "\n",
    "################ 5. Preparing the labels #########################\n",
    "\n",
    "### categorically encode the labels\n",
    "# train_labels = to_categorical(train_labels)\n",
    "# test_labels = to_categorical(test_labels)\n",
    "\n",
    "################ 6. Fit the model ##############################\n",
    "\n",
    "# network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "################ 7. check with Test Data ##############################\n",
    "\n",
    "# test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "# print(\"Test Accu: \", test_acc) ### Test Accuracy: 0.9801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 Scalars (0D tensors)\n",
    "\n",
    "A tensor that contains only one number is called a scalar (or scalar tensor, or 0-dimensional\n",
    "tensor, or 0D tensor). In Numpy, a float32 or float64 number is a scalar tensor (or scalar\n",
    "array)You can display the number of axes of a Numpy tensor via the ndim attribute; a scalar\n",
    "tensor has 0 axes (ndim == 0). The number of axes of a tensor is also called its rank.\n",
    "\n",
    "2.2.2 Vectors (1D tensors)\n",
    "An array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly\n",
    "one axis. \n",
    "\n",
    "2.2.3 Matrices (2D tensors)\n",
    "An array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to rows and columns). The entries from the first axis are called the rows, and the entries from the second axis are called the columns. In the previous example, [5, 78, 2, 34, 0] is the first row of x, and [5, 6, 7] is the first column. \n",
    "\n",
    "2.2.4 3D tensors and higher-dimensional tensors\n",
    "pack such matrices in a new array, you obtain a 3D tensor, which you can visuallyinterpret as a cube of numbers. \n",
    "\n",
    "2.2.5 Key attributes\n",
    "A tensor is defined by three key attributes:\n",
    "1} Number of axes (rank) -  3D tensor has three axes, and a matrix has two axes.\n",
    "2} Shape - It's tuple of integers, how many dimension tesnsor has along each side.\n",
    "3} Data type -  This is the type of the data contained in the tensor\n",
    "\n",
    "    2.6 Displaying the fourth digit\n",
    "    Display the fourth digit in this 3D tensor, using the library Matplotlib. .\n",
    "     Vector data—2D tensors of shape (samples, features)\n",
    " Timeseries data or sequence data—3D tensors of shape (samples, timesteps,\n",
    "features)\n",
    " Images —4D tensors of shape (samples, height, width, channels) or (samples,\n",
    "channels, height, width)\n",
    " Video—5D tensors of shape (samples, frames, height, width, channels) or\n",
    "(samples, frames, channels, height, width)\n",
    "2.2.6 Manipulating tensors in Numpy\n",
    "previous example, we selected a specific digit alongside the first axis using the syntax train_images[i]. Selecting specific elements in a tensor is called tensor slicing.\n",
    "\n",
    "2.2.7 The Notation of data batches \n",
    "The first axis (axis 0, because indexing starts at 0) in all data tensors you’ll come across in deep learning will be the samples axis (sometimes called the samples dimension).\n",
    "deep-learning models don’t process an entire dataset at once; rather, they break the data into small batches. example, one batch of our MNIST digits with batch size of 128. \n",
    "When considering such a batch tensor, the first axis (axis 0) is called the batch axis or\n",
    "batch dimension. \n",
    "\n",
    "2.2.8 Real-world examples of data tensors\n",
    "\n",
    "The data you’ll manipulate will almost always fall into one of the following categories.\n",
    "1} Vector data—2D tensors of shape (samples, features)\n",
    "2} Timeseries data or sequence data—3D tensors of shape (samples, timesteps,\n",
    "features)\n",
    "3} Images —4D tensors of shape (samples, height, width, channels) or (samples,\n",
    "channels, height, width)\n",
    "4} Video—5D tensors of shape (samples, frames, height, width, channels) or\n",
    "(samples, frames, channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[240 253 253 ...   0   0   0]\n",
      "  [ 45 186 253 ...   0   0   0]\n",
      "  [  0  16  93 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[241 243 234 ...   0   0   0]\n",
      "  [143  91  28 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[253 254 253 ...   0   0   0]\n",
      "  [ 72 192 254 ...   0   0   0]\n",
      "  [  0   6 242 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0  31 127 ...   0   0   0]\n",
      "  [ 27 218 252 ...   0   0   0]\n",
      "  [194 253 217 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[ 97 254 252 ...   0   0   0]\n",
      "  [232 181  60 ...   0   0   0]\n",
      "  [ 46   0   0 ...   0   0   0]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]]\n"
     ]
    }
   ],
   "source": [
    "################ 2.1 Scalars (0D tensors) ##############################\n",
    "import numpy as np\n",
    "\n",
    "# x = np.array(12)\n",
    "# x\n",
    "# x.ndim ### Display number of axes, in this case 0\n",
    "# x.shape ### tensor has none{()} dimensions\n",
    "# x.dtype ### tensor contain int32 type data\n",
    "\n",
    "################ 2.2 Vectors (1D tensors) ##############################\n",
    "\n",
    "# x = np.array([1,3,7,11,15]) ### as vector has 5 element, then  it's 5 dimensional vector\n",
    "# x\n",
    "# x.ndim ### Display number of axes, in this case 1\n",
    "# x.shape ### tensor has (5,) dimensions\n",
    "# x.dtype ### tensor contain int32 type data\n",
    "\n",
    "################ 2.3 Matrices (2D tensors) ##############################\n",
    "\n",
    "# x  = np.array([[5, 78, 2, 34, 0],\n",
    "#                [6, 79, 3, 35, 1],\n",
    "#                [7, 80, 4, 36, 2]])\n",
    "# x.ndim ### Display number of axes, in this case 2\n",
    "# x.shape ### tensor has (3, 5) dimensions\n",
    "# x.dtype ### tensor contain int32 type data\n",
    "\n",
    "################ 2.4 3D tensors #########################################\n",
    "\n",
    "# x = np.array([[ [5, 78, 2, 34, 0],\n",
    "#                 [6, 79, 3, 35, 1],\n",
    "#                 [7, 80, 4, 36, 2]],\n",
    "#                [[5, 78, 2, 34, 0],\n",
    "#                 [6, 79, 3, 35, 1],\n",
    "#                 [7, 80, 4, 36, 2]],\n",
    "#                [[5, 78, 2, 34, 0],\n",
    "#                 [6, 79, 3, 35, 1],\n",
    "#                 [7, 80, 4, 36, 2]]])\n",
    "# x.ndim ### Display number of axes, in this case 3\n",
    "# x.shape ### tensor has (3, 3, 5) dimensions\n",
    "# x.dtype ### tensor contain int32 type data\n",
    "\n",
    "################ 2.6 Displaying the fourth digit ##########################\n",
    "\n",
    "### To display the fourth digit in 3D tensor, using Matplotlib library \n",
    "# digit = train_images[4]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(digit, cmap=plt.cm.binary) \n",
    "# plt.show() \n",
    "#### Shows fourth sample from data set\n",
    "\n",
    "################ 2.2.6 Manipulating tensors in Numpy ##########################\n",
    "\n",
    "### selects digits #10 to #100 puts them in an array of shape(90, 28, 28)\n",
    "### specifies a start index and stop index for the slice along each tensor axis.\n",
    "# my_slice = train_images[10:100] \n",
    "# print(my_slice.shape)\n",
    "\n",
    "### Slice the 3D tensor in from 10 to 25 (=15) first index\n",
    "###                        from 2 to 12 (=10) second index i.e. rows\n",
    "###                        from 3 to 17 (=14) third index i.e. columns\n",
    "# my_slice = train_images[10:25, 2:12, 3:17] \n",
    "# print(my_slice.shape)\n",
    "\n",
    "### select 14 × 14 pixels in the bottom-right corner of all images\n",
    "# my_slice = train_images[:, 14:, 14:] \n",
    "# print(my_slice)\n",
    "\n",
    "### To crop the images to patches of 14 × 14 pixels centered in the middle\n",
    "# my_slice = train_images[:, 7: -7, 7: -7]\n",
    "# print(my_slice)\n",
    "\n",
    "################ 2.2.7 The Notation of data batches ##########################\n",
    "\n",
    "# batch = train_images[:128] ### MNIST digits with batch size of 128\n",
    "# print(batch)\n",
    "# batch_2 = train_images[128:256] ### next batch\n",
    "# batch_n = train_images[128 * n: 128 * (n+1)] ### nth batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 The gears of neural networks: tensor operations\n",
    "\n",
    "We can network by stacking Dense layers on top of each other.\n",
    "\n",
    "2.3.1 Element-wise operations\n",
    "\n",
    "The relu and addition are element-wise operations i.e. operations that are applied independently to each entry in the tensors being considered. these operations are highly amenable to massively parallel implementations(vectorized implementations).\n",
    "\n",
    "2.3.2 Broadcasting\n",
    "\n",
    "when the shapes of the two tensors being added differ then the smaller tensor will be broadcasted to match the shape of the larger tensor.\n",
    "Broadcasting consists of two steps\n",
    "1} Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\n",
    "the larger tensor.\n",
    "2} The smaller tensor is repeated alongside these new axes to match the full shape\n",
    "of the larger tensor\n",
    "\n",
    "2.3.3 Tensor dot\n",
    "\n",
    "The dot operation, also called a tensor product is the most common, most useful tensor operation. Contrary to element-wise operations, it combines entries in the input tensors. An element-wise product is done with the * operator in Numpy, Keras, Theano, and TensorFlow. dot uses a different syntax in TensorFlow, but in both Numpy and Keras it’s done using the standard dot operator.\n",
    "\n",
    "2.3.4 Tensor reshaping\n",
    "\n",
    "Reshaping a tensor means rearranging its rows and columns to match a target shape. the reshaped tensor has the same total number of coefficients as the initial tensor. A special case of reshaping that’s commonly encountered is transposition. Transposing a matrix means exchanging its rows and its columns, so that x[i, :] becomes x[:, i].\n",
    "\n",
    "2.3.5 Geometric interpretation of tensor operations\n",
    "\n",
    "elementary geometric operations such as affine transformations, rotations, scaling, and so on can be expressed as tensor operations.\n",
    "\n",
    "2.3.6 A geometric interpretation of deep learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15],\n",
       "       [78],\n",
       "       [28],\n",
       "       [34],\n",
       "       [20],\n",
       "       [61],\n",
       "       [79],\n",
       "       [33],\n",
       "       [35],\n",
       "       [13],\n",
       "       [73],\n",
       "       [80],\n",
       "       [64],\n",
       "       [36],\n",
       "       [29]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ 2.3 neural networks: tensor operations ##########################\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "### Below layer is function which take input as 2D tensor and returns another 2D tensor\n",
    "# keras.layers.Dense(512, activation='relu')\n",
    "\n",
    "### the is function where W is a 2D tensor and b is a vector, both attributes of the layer\n",
    "### three tensor operations here: a dot product (dot) between the input tensor and a tensor \n",
    "### named W; an addition (+) between the resulting 2D tensor and a vector b; and, \n",
    "### finally, a relu operation. relu(x) is max(x, 0).\n",
    "# output = relu(dot(w, input) + b)\n",
    "\n",
    "################ 2.3.1 element-wise relu operation ##########################\n",
    "# x  = np.array([[5, 78, 2, 34, 0],\n",
    "#                [6, 79, 3, 35, 1],\n",
    "#                [7, 80, 4, 36, 2]])\n",
    "# y  = np.array([[15, 78, 28, 34, 20],\n",
    "#                [61, 79, 33, 35, 13],\n",
    "#                [73, 80, 64, 36, 29]])\n",
    "# print(np.maximum((x - y), -1000.))  ### element-wise add/sub/mul in Numpy array\n",
    "# print(x.shape)\n",
    "# print(len(x.shape))\n",
    "# def naive_relu(x):\n",
    "#     assert len(x.shape) == 2  ### x is 2D Numpy tensor\n",
    "#     x = x.copy()              ### avoid overwritting the input tensor\n",
    "#     for i in range(x.shape[0]):\n",
    "#         for j in range(x.shape[1]):\n",
    "#             x[i, j] = max(x[i, j], 0)\n",
    "#     return x        \n",
    "# print(naive_relu(x))\n",
    "\n",
    "################ 2.3.1 element-wise add/sub/mul operation ##########################\n",
    "# def naive_add(x, y):\n",
    "#     assert len(x.shape) == 2\n",
    "#     assert x.shape == y.shape  ### x and y are 2D Numpy tensor\n",
    "#     x = x.copy()               ### to avoid overwriting the input tensor\n",
    "#     for i in range(x.shape[0]):\n",
    "#         for j in range(x.shape[1]):\n",
    "#             x[i, j] *= y[i, j] ### on the same principle, we can do element wise add/sub/mult\n",
    "#     return x        \n",
    "# print(naive_add(x, y))\n",
    "\n",
    "################ 2.3.2 Broadcasting ##############################################\n",
    "\n",
    "# x  = np.array([[5, 78, 2, 34, 0],\n",
    "#                [6, 79, 3, 35, 1]])\n",
    "# y  = np.array([15, 78, 28, 34, 20])\n",
    "# len(x.shape)\n",
    "# len(y.shape)\n",
    "\n",
    "# def naive_add_matrix_and_vector(x, y):\n",
    "#     assert len(x.shape) == 2       ### x is 2D Numpy tensor\n",
    "#     assert len(y.shape) == 1       ### y is 1D Numpy vector\n",
    "#     assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "#     x = x.copy()                   ###  avoid rewritting the input tensor\n",
    "#     for i in range(x.shape[0]):\n",
    "#         for j in range(x.shape[1]):\n",
    "#             x[i, j] *= y[j]        ### similarly, we can elemnt wise sub/mul tensor with vector\n",
    "#     return x        \n",
    "# print(naive_add_matrix_and_vector(x, y))    \n",
    "\n",
    "################ 2.3.3 Tensor dot ##############################################\n",
    "###### dot product of two vectors x and y.\n",
    "# x  = np.array([5, 78, 2, 34, 24])\n",
    "# y  = np.array([15, 78, 28, 34, 20])\n",
    "\n",
    "# def naive_vector_dot(x, y):\n",
    "#     assert len(x.shape) == 1\n",
    "#     assert len(y.shape) == 1\n",
    "#     assert x.shape[0] == y.shape[0]\n",
    "#     z = 0\n",
    "#     for i in range(x.shape[0]):\n",
    "#         z += x[i] * y[i]\n",
    "#     return z\n",
    "# print(naive_vector_dot(x, y))\n",
    "\n",
    "###### dot product of matrix x and vector y\n",
    "# x  = np.array([[5, 78, 2, 34, 0 ],\n",
    "#                [6, 79, 3, 35, 1]])\n",
    "# y  = np.array([15, 78, 28, 34, 20])\n",
    "# len(x.shape)\n",
    "# len(y.shape)\n",
    "\n",
    "# def naive_add_matrix_and_vector(x, y):\n",
    "#     assert len(x.shape) == 2       ### x is 2D Numpy tensor\n",
    "#     assert len(y.shape) == 1       ### y is 1D Numpy vector\n",
    "#     assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "#     z = np.zeros(x.shape[0])       ### vector of 0's with same shape of y\n",
    "#     for i in range(x.shape[0]):\n",
    "#         for j in range(x.shape[1]):\n",
    "#              z[i] += x[i, j] * y[j]\n",
    "#     return z        \n",
    "# print(naive_add_matrix_and_vector(x, y))\n",
    "\n",
    "###### dot product of matrix x(3, 5) and matrix y(5, 3)\n",
    "# x  = np.array([[5, 78, 2, 34, 0],\n",
    "#                [6, 79, 3, 35, 1],\n",
    "#                [7, 80, 4, 36, 2]])\n",
    "# y  = np.array([[15, 78, 28],\n",
    "#                [34, 20, 61],\n",
    "#                [79, 33, 35],\n",
    "#                [13, 73, 80],\n",
    "#                [64, 36, 29]])\n",
    "# def naive_matrix_dot(x, y):\n",
    "#     assert len(x.shape) == 2   ### x numpy matrix\n",
    "#     assert len(y.shape) == 2   ### y numpy matrix\n",
    "#     assert x.shape[1] == y.shape[0] ### 1st dimension of x must be same as 0th dimension of y\n",
    "    \n",
    "#     z = np.zeros((x.shape[0], y.shape[1])) ### matrix of 0 with 1st dimension of x and 2nd of y\n",
    "#     for i in range(x.shape[0]):      ### iterate over x's rows\n",
    "#         for j in range(y.shape[1]):  ### iterate over y's column\n",
    "#             rows_x = x[i, :]\n",
    "#             colums_y = y[:, j]\n",
    "#             z[i, j] = naive_vector_dot(rows_x, colums_y)\n",
    "#     return z\n",
    "# print(naive_matrix_dot(x, y))\n",
    "\n",
    "\n",
    "################ 2.3.4 Tensor reshaping ###########################################\n",
    "# y  = np.array([[15, 78, 28],\n",
    "#                [34, 20, 61],\n",
    "#                [79, 33, 35],\n",
    "#                [13, 73, 80],\n",
    "#                [64, 36, 29]])\n",
    "# # y.shape\n",
    "y.reshape(15,1)\n",
    "# x = np.array([[0., 1.],\n",
    "#               [2., 3.],\n",
    "#               [4., 5.]]) \n",
    "# x.shape\n",
    "# x.reshape(2,3)\n",
    "# x = np.zeros((300,20))\n",
    "# x = np.transpose(x) ### Transposing a matrix means exchanging its rows and its columns\n",
    "# x\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 The engine of neural networks: Gradient-based optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
