{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a network to classify Reuters newswires into 46 mutually exclusive topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as py\n",
    "\n",
    "################# 3.12 Loading the Reuters dataset #######################################\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "# len(train_data)\n",
    "# train_data[10]\n",
    "# train_labels\n",
    "# # len(test_data)\n",
    "\n",
    "################# 3.13 Decoding newswires back to text #######################################\n",
    "\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ''.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "\n",
    "# train_data[0]\n",
    "# train_data[10]\n",
    "\n",
    "################# 3.14 Encoding the data #####################################################\n",
    "\n",
    "def vactorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension)) ### Creates an all-zero matrix of shape (len(sequences), dimension)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. ### Sets specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "x_train = vactorize_sequences(train_data) ### Vectorized training data\n",
    "x_test = vactorize_sequences(test_data) ### Vectorized test data\n",
    "\n",
    "############## Vectorize labels us One-hot encoding #################\n",
    "\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)\n",
    "\n",
    "####################### 3.15 The model definition ######################################\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "####################### 3.16 Compiling the model ######################################\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "####################### 3.17 Setting aside a validation set #############################\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "####################### 3.18 Train the model  ######################################\n",
    "\n",
    "# history = model.fit(partial_x_train, \n",
    "#                     partial_y_train, \n",
    "#                     epochs=20, \n",
    "#                     batch_size=512, \n",
    "#                     validation_data=(x_val, y_val))\n",
    "\n",
    "####################### 3.19 Plotting the training and validation loss ###############\n",
    "\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(1, len(loss)+1)\n",
    "\n",
    "# py.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# py.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# py.title('Training and Validation loss')\n",
    "# py.xlabel('Epochs')\n",
    "# py.ylabel('Loss')\n",
    "# py.legend()\n",
    "# py.show()\n",
    "\n",
    "####################### 3.20 Plotting the training and validation Accuracy ###############\n",
    "\n",
    "# py.clf()\n",
    "\n",
    "# acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "\n",
    "# py.plot(epochs, loss, 'bo', label='Training Accuracy')\n",
    "# py.plot(epochs, val_loss, 'b', label='Validation Accuracy')\n",
    "# py.title('Training and Validation Accuracy')\n",
    "# py.xlabel('Epochs')\n",
    "# py.ylabel('Loss')\n",
    "# py.legend()\n",
    "# py.show()\n",
    "\n",
    "####################### 3.21 Retraining the model #########################################\n",
    "\n",
    "# model.fit(partial_x_train, partial_y_train, epochs=9, validation_data=(x_val, y_val))\n",
    "# results = model.evaluate(x_test, one_hot_test_labels)\n",
    "# results ### Accuracy = 78.85%\n",
    "\n",
    "####################### 3.22 generating prediction for new data ############################\n",
    "\n",
    "prediction = model.predict(x_test)\n",
    "prediction[0].shape ### Each entry in predictions is a vector of length 46\n",
    "np.sum(prediction[0]) ### The coefficients in this vector sum to 1\n",
    "np.argmax(prediction[1]) ### The largest entry is the predicted class with the highest probability\n",
    "# prediction[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
