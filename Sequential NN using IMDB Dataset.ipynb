{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify movie reviews as positive or negative, based on the text content of the reviews. using IMDB dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 4s 177us/step - loss: 0.4476 - acc: 0.8173\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 3s 135us/step - loss: 0.2543 - acc: 0.9100 1s - loss:\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 3s 135us/step - loss: 0.1966 - acc: 0.9298\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 3s 139us/step - loss: 0.1668 - acc: 0.9410\n",
      "25000/25000 [==============================] - 4s 172us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22259216],\n",
       "       [0.9997013 ],\n",
       "       [0.83085567],\n",
       "       ...,\n",
       "       [0.1950918 ],\n",
       "       [0.06340346],\n",
       "       [0.5874053 ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as py\n",
    "\n",
    "################# 3.1 Loading the IMDB dataset #######################################\n",
    "\n",
    "(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# train_data[0]\n",
    "# train_labels[0]\n",
    "\n",
    "# max([max(sequence) for sequence in train_data]) ### max word index\n",
    "# word_index = imdb.get_word_index() ### word_index is a dictionary mapping words to an integer index.\n",
    "# reverse_word_index = dict ([(value, key) for (value, key) in word_index.items()]) ### Reverses it, \n",
    "# ### mapping integer indices to words\n",
    "# decoded_review = ''.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]]) ### Decodes \n",
    "# ### the review. Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices\n",
    "# ### for “padding,” “start of sequence,” and “unknown\".\n",
    "\n",
    "############# 3.2 Encoding the integer sequences into a binary matrix #################\n",
    "\n",
    "def vactorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension)) ### Creates an all-zero matrix of shape (len(sequences), dimension)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. ### Sets specific indices of results[i] to 1s\n",
    "    return results\n",
    "    \n",
    "x_train = vactorize_sequences(train_data) ### Vectorized training data\n",
    "x_test = vactorize_sequences(test_data) ### Vectorized test data\n",
    "\n",
    "# x_train\n",
    "\n",
    "############## Vectorize labels #################\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "# y_train\n",
    "\n",
    "####################### 3.3 The model definition ######################################\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "####################### 3.4 Compiling the model ######################################\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "####################### 3.5,6 configuring custom Optimizer, Loss and metric functino #########\n",
    "\n",
    "# model.compile(optimizer=optimizers.RMSprop(lr=0.001), \n",
    "#               loss=losses.binary_crossentropy, \n",
    "#               metrics=[metrics.binary_accuracy])    \n",
    "              \n",
    "####################### 3.7 setting up validation set ################################      \n",
    "\n",
    "#### create validation set of 10000 sample from training dataset\n",
    "# x_val = x_train[:10000]\n",
    "# partial_x_train = x_train[10000:]\n",
    "# y_val = y_train[:10000]\n",
    "# partial_y_train = y_train[10000:]\n",
    "\n",
    "####################### 3.8 train the model #########################################\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "# history = model.fit(partial_x_train, \n",
    "#                     partial_y_train, \n",
    "#                     epochs=20, \n",
    "#                     batch_size=512, \n",
    "#                     validation_data=(x_val, y_val))\n",
    "\n",
    "### model.fit() returns a History object as member history, which is a dictionary containing\n",
    "### four entries: one per metric that was being monitored during training and during validation\n",
    "\n",
    "# history_dict = history.history\n",
    "# history_dict.keys()\n",
    "\n",
    "####################### 3.9 Plotting the training and validation loss ###############\n",
    "\n",
    "# history_dict = history.history\n",
    "# loss_values = history_dict['loss']\n",
    "# val_loss_values = history_dict['val_loss']\n",
    "# acc = history_dict['acc']\n",
    "# epochs = range(1, len(acc)+1)\n",
    "# py.plot(epochs, loss_values, 'bo', label='Training Loss')\n",
    "# py.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
    "# py.title('Training and Validation Loss')\n",
    "# py.xlabel('Epochs')\n",
    "# py.ylabel('Loss')\n",
    "# py.legend()\n",
    "# py.show()\n",
    "\n",
    "### the training loss decreases with every epoch\n",
    "\n",
    "####################### 3.10 Plotting the training and validation Accuracy ###############\n",
    "\n",
    "# py.clf()\n",
    "# acc_values = history_dict['acc']\n",
    "# val_acc_values = history_dict['val_acc']\n",
    "# py.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "# py.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "# py.title('Training and Validation Accuracy')\n",
    "# py.xlabel('Epochs')\n",
    "# py.ylabel('Loss')\n",
    "# py.legend()\n",
    "# py.show()\n",
    "\n",
    "### the training accuracy increases with every epoch\n",
    "\n",
    "####################### 3.11 Retraining the model #########################################\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "results ### with 4 epoch, accuracy of 88% is achieved\n",
    "\n",
    "model.predict(x_test) ### Using a trained network to generate predictions on new data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
