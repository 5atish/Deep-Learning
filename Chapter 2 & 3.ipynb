{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "###################### 2.1 Loading the MNIST dataset in Keras ###################################\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "len(train_images)\n",
    "train_images.shape\n",
    "# train_images\n",
    "train_labels.shape\n",
    "\n",
    "test_images.shape\n",
    "# test_labels.shape\n",
    "\n",
    "\n",
    "# my_slice = train_images[:,0:14, 0:14] # select 14 × 14 pixels in the bottom-right corner of all images\n",
    "my_slice = train_images[:,7:-7, 7:-7] # to crop the images to patches of 14 × 14 pixels centered in the middlle\n",
    "print(my_slice.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### 2.2 The network architecture ###############################################\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### 2.3 The compilation step ###############################################\n",
    "\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### 2.4 Preparing the image data ###############################################\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32')/255 \n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.2563 - acc: 0.9264\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1042 - acc: 0.9695\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0699 - acc: 0.9790\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.0503 - acc: 0.9848\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.0380 - acc: 0.9883\n",
      "10000/10000 [==============================] - 1s 86us/step\n",
      "Test Accuaracy:  0.9812\n"
     ]
    }
   ],
   "source": [
    "###################### 2.5 Preparing the labels and train model #####################################\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"Test Accuaracy: \", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-c2f300e51dfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[0;32m   3203\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3204\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3205\u001b[1;33m                         **kwargs)\n\u001b[0m\u001b[0;32m   3206\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3207\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1853\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1855\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5485\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    651\u001b[0m         if not (self._A.ndim == 2\n\u001b[0;32m    652\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m--> 653\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################### 2.6 Displaying the fourth digit ############################################\n",
    "\n",
    "digit = train_images[6]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 756., 9796.,  387.])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "# x  = np.array([[5, 78, 2, 34, 7],\n",
    "#                [6, 79, 3, 35, 1],\n",
    "#                [7, 80, 4, 36, 2]])\n",
    "\n",
    "# y  = np.array([[5, 78, 2, 34, 4],\n",
    "#                [6, 79, 3, 35, 1],\n",
    "#                [7, 80, 4, 36, 2]])\n",
    "\n",
    "\n",
    "\n",
    "# print(len(y.shape))\n",
    "# def naive_relu(x):\n",
    "#     assert len(x.shape)==2    \n",
    "#     x = x.copy()\n",
    "#     for i in range(x.shape[0]):\n",
    "#         for j in range(x.shape[1]):\n",
    "#             x[i, j] = max(x[i, j], 0)\n",
    "#     return x\n",
    "# naive_relu(x)\n",
    "\n",
    "# def naive_add(x, y):\n",
    "#     assert len(x.shape)==2\n",
    "#     assert x.shape == y.shape\n",
    "    \n",
    "#     x  = x.copy()\n",
    "#     for i in range(x.shape[0]):\n",
    "#         for j in range(y.shape[1]):\n",
    "#             x[i, j] /= y[i, j]\n",
    "#     return x\n",
    "\n",
    "# naive_add(x, y)\n",
    "\n",
    "########################## the dot product of two vectors #######################################\n",
    "\n",
    "# x  = np.array([5, 78, 2, 34, 7])\n",
    "# y = np.array([6, 79, 3, 35, 1])\n",
    "\n",
    "# def naive_vector_dot(x, y):\n",
    "#     assert len(x.shape)==1\n",
    "#     assert len(y.shape)==1\n",
    "#     assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "#     z = 0.\n",
    "#     for i in range(x.shape[0]):\n",
    "#         z += x[i] * y[i]\n",
    "#     return z\n",
    "\n",
    "# print(naive_vector_dot(x, y))\n",
    "\n",
    "########################## the dot product of matrices and vector #############################\n",
    "\n",
    "x  = np.array([[5, 78, 2, 34, 7],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]])\n",
    "y = np.array([6, 79, 3, 35, 1])\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape)==2\n",
    "    assert len(y.shape)==1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[i]\n",
    "    return z\n",
    "\n",
    "naive_matrix_vector_dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 78  2]\n",
      " [34  7  6]\n",
      " [79  3 35]\n",
      " [ 1  7 80]\n",
      " [ 4 36  2]]\n"
     ]
    }
   ],
   "source": [
    "########################## the dot product of two matrices #############################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x  = np.array([[5, 78, 2, 34, 7],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]])\n",
    "# y = np.array([[5, 78, 2],\n",
    "#               [6, 79, 3],\n",
    "#               [7, 80, 4],\n",
    "#               [7, 80, 4],\n",
    "#               [5, 78, 2]])\n",
    "y = x.transpose()\n",
    "\n",
    "# print((y.shape[0]))\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape)==1\n",
    "    assert len(y.shape)==1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            rows_x = x[i, :]\n",
    "            colums_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(rows_x, colums_y)\n",
    "    return z\n",
    "\n",
    "# naive_matrix_dot(x, y)\n",
    "\n",
    "x.shape\n",
    "print(x.reshape(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 4s 164us/step - loss: 0.4484 - acc: 0.8172\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 3s 136us/step - loss: 0.2546 - acc: 0.9094\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 3s 135us/step - loss: 0.1968 - acc: 0.9297\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 3s 129us/step - loss: 0.1668 - acc: 0.9414\n",
      "25000/25000 [==============================] - 4s 143us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2261905 ],\n",
       "       [0.99971753],\n",
       "       [0.82546026],\n",
       "       ...,\n",
       "       [0.1927456 ],\n",
       "       [0.06373461],\n",
       "       [0.57737345]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### 3.1 Loading the IMDB dataset ###############################################\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "test_data[0]\n",
    "\n",
    "###################### 3.2 Encoding the integer sequences into a binary matrix ######################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    result = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        result[i, sequence] = 1\n",
    "    return result\n",
    "\n",
    "x_test = vectorize_sequences(test_data)\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# x_test[0]\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "###################### 3.3 The model definition ###############################################\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "###################### 3.4 Compiling the model ###############################################\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "###################### 3.7 Setting aside a validation set ###############################################\n",
    "\n",
    "#### create a validation set by setting apart 10,000 samples from the original training data\n",
    "\n",
    "# x_val = x_train[:10000]\n",
    "# y_val = y_train[:10000]\n",
    "\n",
    "# partial_x_train = x_train[10000:]\n",
    "# partial_y_train = y_train[10000:]\n",
    "\n",
    "###################### 3.8 Training your model ###############################################\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))\n",
    "\n",
    "# history_dict = history.history\n",
    "# history_dict.keys()\n",
    "\n",
    "###################### 3.9 Plotting the training and validation loss ###############################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# history_dict = history.history\n",
    "# loss_values = history_dict['loss']\n",
    "# val_loss_values = history_dict['val_loss']\n",
    "# acc = history_dict['acc']\n",
    "# epochs = range(1, len(acc)+1)\n",
    "\n",
    "# plt.plot(epochs, loss_values, 'bo', label='Training Loss')\n",
    "# plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n",
    "# plt.title('Training & Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "\n",
    "##### the training loss decreases with every epoch\n",
    "\n",
    "# acc_values = history_dict['acc']\n",
    "# val_acc_values = history_dict['val_acc']\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(epochs, acc, 'bo', label='Training Acc')\n",
    "# plt.plot(epochs, val_acc_values, 'b', label='Validation Acc')\n",
    "# plt.title('Training & Validation Acc')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "\n",
    "##### the training accuracy increases with every epoch\n",
    "\n",
    "\n",
    "###################### 3.11 Retraining a model from scratch ###############################################\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "# result = model.evaluate(x_test, y_test)\n",
    "# result ### This approach achieves an accuracy of 88%.\n",
    "\n",
    "######### Using a trained network to generate predictions on new data \n",
    "\n",
    "model.predict(x_test)\n",
    "### the network is confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### 3.12 Loading the Reuters dataset ###############################################\n",
    "\n",
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "# train_data.shape\n",
    "# train_labels[1]\n",
    "\n",
    "# test_data[1]\n",
    "# test_labels[3]\n",
    "\n",
    "###################### 3.13 Decoding newswires back to text ###############################################\n",
    "\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]]) ## the indices are offset by 3 \n",
    "### because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”\n",
    "\n",
    "train_labels[10]\n",
    "\n",
    "###################### 3.14 Encoding the data #############################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# def vectorize_sequences(sequences, dimension=10000):\n",
    "#     results = np.zeros((len(sequences), dimension))\n",
    "#     for i, sequence in enumerate(sequences):\n",
    "#         results[i, sequence] = 1\n",
    "#     return results\n",
    "\n",
    "# x_train = vectorize_sequences(train_data)\n",
    "# x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# train_labels\n",
    "\n",
    "# def to_one_hot(labels, dimension=46):\n",
    "#     results = np.zeros((len(labels), dimension))\n",
    "#     for i, label in enumerate(labels):\n",
    "#         results[i, label] = 1\n",
    "#     return results\n",
    "\n",
    "# x_labels = to_one_hot(train_labels)\n",
    "# y_labels = to_one_hot(test_labels)\n",
    "\n",
    "###################### 3.15 Model definition #############################################################\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "\n",
    "###################### 3.16 Compiling the model #############################################################\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "###################### 3.17 Setting aside a validation set ##################################################\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "y_val = x_labels[:1000]\n",
    "\n",
    "partial_x_train = x_train[1000:]\n",
    "partial_y_train = x_labels[1000:]\n",
    "\n",
    "###################### 3.18 Training the model ##############################################################\n",
    "\n",
    "# history  = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))\n",
    "\n",
    "# history.dict = history.history\n",
    "\n",
    "###################### 3.18 Training the model ##############################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loss = history_dict['loss']\n",
    "# val_loss = history_dict['val_loss']\n",
    "# epochs = range(1, len(loss)+1)\n",
    "# plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "# plt.title('Training & Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# # Plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# acc = history_dict['acc']\n",
    "# val_acc = history_dict['val_acc']\n",
    "# epochs = range(1, len(acc)+1)\n",
    "# plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "# plt.title('Training & Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# # plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "###################### 3.21 Retraining a model from scratch ##############################################\n",
    "\n",
    "# model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val))\n",
    "# results = model.evaluate(x_test, y_labels)\n",
    "# results ### Accuracy 78.85%\n",
    "\n",
    "###################### 3.22 Generating predictions for new data ##########################################\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "predictions[0].shape ### predictions is a vector of length 46\n",
    "# np.sum(predictions[0]) ### The coefficients in this vector sum to 1:\n",
    "np.max(predictions[0])\n",
    "np.argmax(predictions[15]) ### The largest entry is the predicted class i.e. the class with the highest probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5378064080780627"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### 3.24 Loading the Boston housing dataset ##########################################\n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "# train_data[0]\n",
    "# test_data[0]\n",
    "\n",
    "# train_targets[0]\n",
    "# test_targets[0]\n",
    "\n",
    "###################### 3.25 Normalizing the data #######################################################\n",
    "\n",
    "### To deal with wildly different range data, It is best practice to do feature wise normalization \n",
    "### i.e. for each feature in the input data subtract the mean of the feature anddivide by the standard \n",
    "### deviation, so that the feature is centered around 0 and has a unit standard deviation.\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "\n",
    "train_data -= mean\n",
    "train_data /= std\n",
    "\n",
    "### the quantities used for normalizing the test data are computed using the training data\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std\n",
    "\n",
    "###################### 3.26 Model definition #######################################################\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "###################### 3.27 K-fold validation #######################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len((train_data))//k\n",
    "# num_epochs = 100\n",
    "# all_score = []\n",
    "\n",
    "# for i in range(k):\n",
    "#     print('processing fold#', i)\n",
    "#     val_data = train_data[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "#     val_target = train_targets[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "    \n",
    "#     partial_train_data = np.concatenate([train_data[:i * num_val_samples],train_data[(i + 1) * num_val_samples:]], axis=0)\n",
    "#     partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "     \n",
    "#     model = build_model()\n",
    "#     model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "#     val_mse, val_mae = model.evaluate(val_data, val_target, verbose=0)\n",
    "#     all_score.append(val_mae)\n",
    "    \n",
    "# all_score ### To see individual score of k fold validation\n",
    "# np.mean(all_score) ### average of all k fold validation scores\n",
    "\n",
    "###################### 3.28 Saving the validation logs at each fold ##########################################\n",
    "\n",
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "\n",
    "# for i in range(k):\n",
    "#     print('processing fold#', i)\n",
    "#     val_data = train_data[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "#     val_target = train_targets[i * num_val_samples:(i + 1) * num_val_samples]\n",
    "    \n",
    "#     partial_train_data = np.concatenate([train_data[:i * num_val_samples],train_data[(i + 1) * num_val_samples:]], axis=0)\n",
    "#     partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "     \n",
    "#     model = build_model()\n",
    "#     history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_target), epochs=num_epochs, batch_size=1, verbose=0)\n",
    "#     mae_history = history.history['val_mean_absolute_error']\n",
    "#     all_mae_histories.append(mae_history)\n",
    "    \n",
    "# average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]   \n",
    "# average_mae_history\n",
    "\n",
    "###################### 3.30 Plotting validation scores ###################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Validation MAE')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "###################### 3.32 Training the final model ###################################################\n",
    "\n",
    "model = build_model()\n",
    "model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "test_mae_score #### 2.537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 4)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "99 // 4\n",
    "range(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
